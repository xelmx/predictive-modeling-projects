{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries and Preparing the Dataset for Employability Prediction"
      ],
      "metadata": {
        "id": "Xe59-VLre9E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Essential Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-learn for ML tasks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer # To handle different column types\n",
        "from sklearn.pipeline import Pipeline # To chain preprocessing and model\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# For Colab specific tasks (like mounting Drive)\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access the dataset\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"Drive mounted successfully.\")"
      ],
      "metadata": {
        "id": "7N7O8IyOfGOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Initial Exploration"
      ],
      "metadata": {
        "id": "B-m_Fhi1frt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XTC6lFUrfyzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset - make sure the path is correct for your Drive setup!\n",
        "try:\n",
        "    # Path provided by the user\n",
        "    dataset_path = '/content/drive/My Drive/Colab Notebooks/Student-Employability-Dataset.csv'\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    print(f\"Dataset '{dataset_path}' loaded successfully.\")\n",
        "    print(\"\\nHead of the dataset:\")\n",
        "    display(df.head()) # Use display() for cleaner output in notebooks\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Dataset not found at '{dataset_path}'. Please check the file path.\")\n",
        "    # Stop execution if file isn't found.\n",
        "    raise SystemExit(\"Dataset file not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the dataset: {e}\")\n",
        "    raise SystemExit(\"Error during dataset loading.\")\n"
      ],
      "metadata": {
        "id": "AdRCaXesfwWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q5g3FRM8fzs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get initial shape and column names\n",
        "print(\"Initial shape of the dataset:\", df.shape)\n",
        "print(\"\\nInitial columns of the dataset:\\n\", df.columns.tolist())\n",
        "\n",
        "# Code Cell\n",
        "# Check data types and missing values\n",
        "print(\"\\nDataset Info:\")\n",
        "df.info()\n",
        "\n",
        "# Code Cell\n",
        "# Check unique values and missing values per column\n",
        "print(\"\\nUnique values and Missing values per column:\")\n",
        "for column in df.columns:\n",
        "    print(f\"- {column}: Unique values = {df[column].nunique()}, Missing values = {df[column].isnull().sum()}\")"
      ],
      "metadata": {
        "id": "UTGoB1Qff0SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Data Preprocessing for Employability Prediction\n",
        "\n",
        " 1.  Dropping irrelevant columns.\n",
        " 2.  Handling Missing Values (if any).\n",
        " 3.  Defining features (X) and the target (y).\n",
        " 4.  Encoding the categorical target variable ('CLASS').\n",
        " 5.  Identifying numerical features for scaling.\n",
        " 6.  Scaling numerical features (MLPs are sensitive to feature scaling)."
      ],
      "metadata": {
        "id": "sXtqMu0Tg0C2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KRWVYff2g9zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Dropping Irrelevant Columns (as specified by user)\n",
        "# Ensure the column 'Name of Student' exists before trying to drop it.\n",
        "columns_to_drop = ['Name of Student']\n",
        "actual_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
        "\n",
        "if actual_columns_to_drop:\n",
        "    df = df.drop(columns=actual_columns_to_drop)\n",
        "    print(f\"Dropped columns: {actual_columns_to_drop}\")\n",
        "else:\n",
        "    print(f\"Specified column(s) to drop ({columns_to_drop}) not found in the dataset's current columns: {df.columns.tolist()}.\")\n",
        "print(\"Shape after attempting to drop columns:\", df.shape)\n",
        "print(\"Remaining columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "KpQXCiNjg_Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sX4vs03xhC_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Handling Missing Values (Illustrative - Adapt as needed)\n",
        "# This section checks for missing values in potential feature columns.\n",
        "# IMPORTANT: The target column is assumed to be 'CLASS'. If it's different, update it below.\n",
        "TARGET_COLUMN_NAME = 'CLASS' # Or 'Employable', etc. - check your CSV\n",
        "\n",
        "if TARGET_COLUMN_NAME not in df.columns:\n",
        "    print(f\"Error: Target column '{TARGET_COLUMN_NAME}' not found in the DataFrame after dropping columns.\")\n",
        "    print(f\"Available columns are: {df.columns.tolist()}\")\n",
        "    raise SystemExit(f\"Target column '{TARGET_COLUMN_NAME}' missing.\")\n",
        "\n",
        "potential_feature_cols = df.drop(TARGET_COLUMN_NAME, axis=1, errors='ignore').columns\n",
        "print(f\"\\nChecking for missing values in potential feature columns: {potential_feature_cols.tolist()}\")\n",
        "missing_values_in_features = df[potential_feature_cols].isnull().sum().sum()\n",
        "\n",
        "if missing_values_in_features > 0:\n",
        "    print(f\"Warning: Found {missing_values_in_features} missing values in feature columns.\")\n",
        "    # Example: Fill with median for numerical features\n",
        "    for col in df.select_dtypes(include=np.number).columns:\n",
        "        if col != TARGET_COLUMN_NAME and df[col].isnull().sum() > 0 : # Don't impute target here\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "            print(f\"Filled NaNs in numerical feature '{col}' with its median.\")\n",
        "    # Example: Fill with mode for categorical features\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "         if col != TARGET_COLUMN_NAME and df[col].isnull().sum() > 0 : # Don't impute target here\n",
        "            df[col] = df[col].fillna(df[col].mode()[0])\n",
        "            print(f\"Filled NaNs in categorical feature '{col}' with its mode.\")\n",
        "else:\n",
        "    print(\"No missing values found in potential feature columns (excluding target).\")\n",
        "\n",
        "if df[TARGET_COLUMN_NAME].isnull().sum() > 0:\n",
        "    print(f\"Warning: Target column '{TARGET_COLUMN_NAME}' has {df[TARGET_COLUMN_NAME].isnull().sum()} missing values.\")\n",
        "    # Option 1: Drop rows with missing target\n",
        "    df.dropna(subset=[TARGET_COLUMN_NAME], inplace=True)\n",
        "    print(f\"Dropped rows with missing target. New shape: {df.shape}\")\n",
        "    # Option 2: Impute target (less common, depends on problem) - not shown here\n"
      ],
      "metadata": {
        "id": "DGoKyjvyhDUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EPqKbMtshJze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define features (X) and target (y)\n",
        "# Ensure TARGET_COLUMN_NAME is correct based on your CSV file.\n",
        "X = df.drop(TARGET_COLUMN_NAME, axis=1)\n",
        "y = df[TARGET_COLUMN_NAME]\n",
        "\n",
        "print(f\"\\nTarget '{TARGET_COLUMN_NAME}' distribution (original):\")\n",
        "print(y.value_counts())\n",
        "print(\"\\nFeatures (X) shape:\", X.shape)\n",
        "print(\"Target (y) shape:\", y.shape)\n",
        "\n",
        "if X.empty and not df.empty:\n",
        "    raise ValueError(\"Features (X) are empty. This might be due to incorrect column dropping or an empty input file after dropping the target column.\")\n",
        "if X.shape[1] == 0 and df.shape[0] > 0 :\n",
        "    print(f\"Warning: Features DataFrame X has no columns. Check if all columns were dropped or if '{TARGET_COLUMN_NAME}' was the only other column.\")\n"
      ],
      "metadata": {
        "id": "oTSjMq85hKOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JfUFFTvIhNGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Encoding the Target Variable\n",
        "# The target column (e.g., 'CLASS') is likely categorical ('Employable', 'LessEmployable').\n",
        "# We need to convert it to numerical form for the MLP.\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f\"\\nEncoded target '{TARGET_COLUMN_NAME}' distribution:\")\n",
        "# Show mapping\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    print(f\"'{class_name}' -> {label_encoder.transform([class_name])[0]}\")\n",
        "print(pd.Series(y_encoded).value_counts()) # Show distribution of encoded values\n",
        "\n",
        "# Store class names for later use in evaluation\n",
        "encoded_class_names = label_encoder.classes_\n"
      ],
      "metadata": {
        "id": "A12cVnQqhNtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RqF8yo8nhcKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Encoding the Target Variable\n",
        "# The target column (e.g., 'CLASS') is likely categorical ('Employable', 'LessEmployable').\n",
        "# We need to convert it to numerical form for the MLP.\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f\"\\nEncoded target '{TARGET_COLUMN_NAME}' distribution:\")\n",
        "# Show mapping\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    print(f\"'{class_name}' -> {label_encoder.transform([class_name])[0]}\")\n",
        "print(pd.Series(y_encoded).value_counts()) # Show distribution of encoded values\n",
        "\n",
        "# Store class names for later use in evaluation\n",
        "encoded_class_names = label_encoder.classes_\n"
      ],
      "metadata": {
        "id": "sKVxDtechcjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9SUlY56whgjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Identify numerical and categorical input features for scaling/encoding\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist() # These are input features, not the target\n",
        "\n",
        "print(\"\\nNumerical input columns for preprocessing:\", numerical_cols)\n",
        "print(\"Categorical input columns for preprocessing:\", categorical_cols)\n",
        "\n",
        "if not numerical_cols and not categorical_cols and not X.empty:\n",
        "     print(\"Warning: No numerical or categorical input features identified in X. Check dtypes. All columns might be of an unexpected type or already processed.\")\n",
        "elif X.empty and df.shape[0] > 0 :\n",
        "    print(\"Warning: Features DataFrame X is empty. This could be due to all columns being dropped or used as target.\")\n",
        "\n",
        "\n",
        "# Visualize distributions of identified numerical features\n",
        "if numerical_cols:\n",
        "    print(\"\\nHistograms of Numerical Features:\")\n",
        "    X[numerical_cols].hist(bins=15, figsize=(15,10))\n",
        "    plt.suptitle('Histograms of Student Employability Features', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nNo numerical features to visualize in X.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "j9ZiaDcJhkBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u-VAx-fXho9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Create preprocessing pipeline\n",
        "# Numerical features will be scaled.\n",
        "# Categorical input features (if any) will be One-Hot Encoded.\n",
        "\n",
        "transformers_list = []\n",
        "if numerical_cols:\n",
        "    transformers_list.append(('num', StandardScaler(), numerical_cols))\n",
        "if categorical_cols:\n",
        "    from sklearn.preprocessing import OneHotEncoder # ensure import\n",
        "    transformers_list.append(('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols))\n",
        "\n",
        "if not transformers_list:\n",
        "    print(\"Warning: No transformers created as no numerical or categorical columns were identified in X for preprocessing.\")\n",
        "    # Create a dummy preprocessor that does nothing if X is meant to be used as-is\n",
        "    # This is unlikely for MLP but handles the edge case.\n",
        "    from sklearn.preprocessing import FunctionTransformer\n",
        "    preprocessor = FunctionTransformer(lambda x: x) # Passthrough\n",
        "else:\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=transformers_list,\n",
        "        remainder='passthrough' # Keeps columns not specified, if any\n",
        "    )"
      ],
      "metadata": {
        "id": "lKd-7kjThpbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Data and Building the MLP for Employability Prediction"
      ],
      "metadata": {
        "id": "eN2QI87UhsEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6Fyv5K40h3Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets using the ENCODED target\n",
        "# We split X *before* applying the ColumnTransformer to avoid data leakage.\n",
        "\n",
        "# Stratify helps maintain class proportions, especially important for smaller or imbalanced datasets.\n",
        "# Adjust test_size as needed.\n",
        "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(\n",
        "    X, y_encoded,\n",
        "    test_size=0.25, # Common split ratio\n",
        "    random_state=42,\n",
        "    stratify=y_encoded if len(np.unique(y_encoded)) > 1 and len(y_encoded) > sum(np.unique(y_encoded, return_counts=True)[1]) else None # Stratify if possible\n",
        ")\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train_encoded shape:\", y_train_encoded.shape)\n",
        "print(\"y_test_encoded shape:\", y_test_encoded.shape)\n",
        "\n",
        "if X_train.empty or X_test.empty:\n",
        "    print(\"Warning: Training or testing set is empty. This is likely due to a very small dataset size after splitting.\")\n",
        "    print(\"MLP training might fail or be meaningless.\")\n"
      ],
      "metadata": {
        "id": "swYGAIZKhsgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UZ5w8Zb0h5bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the MLP model pipeline\n",
        "# The MLPClassifier will predict the encoded binary classes (e.g., 0 or 1).\n",
        "\n",
        "# Note on MLP Hyperparameters:\n",
        "# These are example values. For optimal performance, they should be tuned (e.g., using GridSearchCV).\n",
        "# For very small datasets, a simpler model or a much smaller MLP is recommended.\n",
        "mlp = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', MLPClassifier(\n",
        "        hidden_layer_sizes=[100], # Example: One hidden layer with 100 neurons\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        max_iter=1000,\n",
        "        learning_rate_init=0.001,\n",
        "        alpha=0.0001,           # L2 regularization\n",
        "        random_state=42,\n",
        "        early_stopping=True,    # Can help prevent overfitting and reduce training time\n",
        "        validation_fraction=0.1, # Proportion of training data to set aside as validation set for early stopping\n",
        "        n_iter_no_change=10,    # Number of iterations with no improvement to wait before stopping\n",
        "        verbose=False           # Set to True to see training progress\n",
        "    ))\n",
        "])\n",
        "\n",
        "print(\"MLP Pipeline created for Employability Prediction.\")\n",
        "\n",
        "# Check if training data is available\n",
        "if not X_train.empty and not pd.Series(y_train_encoded).empty:\n",
        "    print(f\"\\nTraining the MLP model to predict '{TARGET_COLUMN_NAME}'...\")\n",
        "    try:\n",
        "        mlp.fit(X_train, y_train_encoded)\n",
        "        print(\"Training finished.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during MLP training: {e}\")\n",
        "        print(\"This could be due to issues with preprocessing, data types, or insufficient data.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping MLP training due to empty training set. This usually happens with extremely small initial datasets.\")"
      ],
      "metadata": {
        "id": "dvyKsrcGiC1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation (Employability Prediction)"
      ],
      "metadata": {
        "id": "vz1zGawHiK7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IxZfuY8oiRg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test data\n",
        "if not X_test.empty and not pd.Series(y_test_encoded).empty and hasattr(mlp.named_steps.get('classifier'), 'predict'):\n",
        "    try:\n",
        "        y_pred_encoded = mlp.predict(X_test)\n",
        "\n",
        "        # Evaluate the model\n",
        "        accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
        "        print(f\"\\nOverall Accuracy on Test Set: {accuracy:.4f}\")\n",
        "\n",
        "        # Classification Report\n",
        "        # Use the class names learned by LabelEncoder\n",
        "        print(\"\\nClassification Report:\")\n",
        "        # zero_division=0 handles cases where a class has no predicted samples in a batch.\n",
        "        print(classification_report(y_test_encoded, y_pred_encoded, target_names=encoded_class_names, zero_division=0))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        conf_matrix = confusion_matrix(y_test_encoded, y_pred_encoded, labels=range(len(encoded_class_names)))\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(conf_matrix)\n",
        "\n",
        "        # Optional: Visualize the confusion matrix\n",
        "        if len(encoded_class_names) > 0 :\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                        xticklabels=encoded_class_names, yticklabels=encoded_class_names)\n",
        "            plt.title(f'Confusion Matrix for {TARGET_COLUMN_NAME} Prediction')\n",
        "            plt.xlabel('Predicted Label')\n",
        "            plt.ylabel('True Label')\n",
        "            plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during model evaluation: {e}\")\n",
        "else:\n",
        "    print(\"\\nSkipping model evaluation due to empty test set or untrained/invalid model.\")"
      ],
      "metadata": {
        "id": "_CS4TBRZiTW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single Sample Prediction (Employability Prediction)"
      ],
      "metadata": {
        "id": "vtJ6C0jxibc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DJ0vTKMcifZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict for a single sample\n",
        "if not X_test.empty and hasattr(mlp.named_steps.get('classifier'), 'predict'):\n",
        "    # Let's take the first sample from the original X_test (before pipeline's preprocessing)\n",
        "    sample_to_predict_raw = X_test.iloc[:1]\n",
        "    print(\"\\nRaw Test Sample for Prediction:\\n\", sample_to_predict_raw)\n",
        "\n",
        "    try:\n",
        "        # Predict probability for this sample using the full pipeline\n",
        "        # Output will be probabilities for [class_0, class_1, ...]\n",
        "        prob = mlp.predict_proba(sample_to_predict_raw)\n",
        "        print(f\"\\nProbability Results for Sample (Classes: {encoded_class_names}):\", prob)\n",
        "\n",
        "        # The predicted class is the index with the highest probability\n",
        "        predicted_class_encoded = mlp.predict(sample_to_predict_raw)[0]\n",
        "\n",
        "        # Interpret the prediction using the label_encoder\n",
        "        predicted_label_text = label_encoder.inverse_transform([predicted_class_encoded])[0]\n",
        "\n",
        "        print(\"\\nPredicted Class (encoded): \", predicted_class_encoded)\n",
        "        print(f\"Predicted Label for '{TARGET_COLUMN_NAME}': \", predicted_label_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during single sample prediction: {e}\")\n",
        "        print(\"This might happen if the model was not trained due to insufficient data or preprocessing issues.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping single sample prediction due to empty test set or untrained/invalid model.\")"
      ],
      "metadata": {
        "id": "bCWx5dSNlHR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}